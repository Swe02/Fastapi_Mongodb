{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ly8N0LgJ91L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers gradio langchain sentence-transformers langchain_chroma\n",
        "\n",
        "!pip install langchain-community\n",
        "\n",
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2PhM_DS_KZY7",
        "outputId": "d51b263b-bd92-4854-ef57-b1fd790c7994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.23.2 (from pandas)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ec9bb6c4472a432ca5cd8ecc4930e864",
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Using cached gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Using cached gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jinja2<4.0 (from gradio)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall pandas\n",
        "!pip install --force-reinstall gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW7Zz_r9KzkC",
        "outputId": "d6ae335a-4a09-4928-ce17-a78dd01552c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/Knowledge-base.zip\n",
            "   creating: /content/Knowledge-base/\n",
            "   creating: /content/Knowledge-base/disease-info/\n",
            "  inflating: /content/Knowledge-base/disease-info/disease_treatment_info_expanded.md  \n",
            "  inflating: /content/Knowledge-base/disease-info/first_aid_emergency.md  \n",
            "  inflating: /content/Knowledge-base/disease-info/health_tips_preventive_care.md  \n",
            "  inflating: /content/Knowledge-base/disease-info/medication_info_expanded.md  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/Knowledge-base.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuJJieCBK9-e",
        "outputId": "798ac179-81ac-4b53-d642-c13892002021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `chatbot` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `chatbot`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Configuration\n",
        "DB_NAME = \"vector_db\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Load the tokenizer and model for text generation\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Set up the pipeline for text generation\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "# Load documents from the knowledge base\n",
        "folders = glob.glob(\"/content/Knowledge-base/*\")\n",
        "text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "documents = []\n",
        "for folder in folders:\n",
        "    doc_type = os.path.basename(folder)\n",
        "    loader = DirectoryLoader(folder, glob=\"*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
        "    folder_docs = loader.load()\n",
        "\n",
        "    if folder_docs:\n",
        "        documents.extend(folder_docs)\n",
        "    else:\n",
        "        print(f\"Warning: No documents found in {folder}\")\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "\n",
        "# Split text into chunks for processing\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create the embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "\n",
        "# Initialize Chroma for vector storage\n",
        "if os.path.exists(DB_NAME):\n",
        "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
        "\n",
        "# Create and persist vector store\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Memory setup for conversation history\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# Define the chat function for conversation and context retrieval\n",
        "def chat(question, history):\n",
        "    # Retrieve relevant context from the vector store\n",
        "    context = retriever.get_relevant_documents(question)\n",
        "\n",
        "    if not context:\n",
        "        context_text = \"No relevant context found. Answer to the best of your ability.\"\n",
        "    else:\n",
        "        context_text = \"\\n\".join([doc.page_content for doc in context])\n",
        "\n",
        "    # Prepare input text for the model\n",
        "    input_text = f\"### Context:\\n{context_text}\\n\\n### Question:\\n{question}\\n\\n### Answer:\"\n",
        "\n",
        "    print(f\"Debug - Input to model: {input_text}\")  # Debugging output\n",
        "\n",
        "    # Generate the model response\n",
        "    response = text_gen_pipeline(input_text)[0][\"generated_text\"]\n",
        "\n",
        "    # Clean up the response to remove input_text\n",
        "    return response.replace(input_text, \"\").strip()\n",
        "\n",
        "# Set up Gradio interface for chat\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(debug=True, inbrowser=True)\n"
      ],
      "metadata": {
        "id": "JohGVOob5Epg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}